1+1
Sys.setenv(LANG="en_US.UTF-8")
Sys.setenv(LC_ALL="en_US.UTF-8")
df <- read.table(text = "       Input Rtime Rcost Rsolutions  Btime Bcost
1   12-proc.     1    36     614425     40    36
2   15-proc.     1    51     534037     50    51
3    18-proc     5    62    1843820     66    66
4    20-proc     4    68    1645581 104400    73
5 20-proc(l)     4    64    1658509  14400    65
6    21-proc    10    78    3923623 453600    82",header = TRUE,sep = "")
View(df)
dfm <- melt(df[,c('Input','Rtime','Btime')],id.vars = 1)
dfm <- melt(df[,c('Input','Rtime','Btime')],id.vars = 1)
install.packages("melt")
dfm <- melt(df[,c('Input','Rtime','Btime')],id.vars = 1)
install.packages("reshape2")
dfm <- melt(df[,c('Input','Rtime','Btime')],id.vars = 1)
install.packages("reshape")
dfm <- melt(df[,c('Input','Rtime','Btime')],id.vars = 1)
install.packages("reshape2")
df<-read.csv('F.csv',header=T)
library(reshape2)
dfm <- melt(df[,c('Dataset...Algorithm.','Org','B.Bag','B.Ins')],id.vars = 1)
colnames(dfm) <- c("Dataset.Algorithm.", "variable","F1-Score")
library(ggplot2)
library(Cairo)
Cairo(file="F.png",type="png",units="in",width=10,height=7,pointsize=12,dpi=300)
ggplot(dfm,aes(x = Dataset.Algorithm. ,y = F1-Score)) +
geom_bar(aes(fill = variable),position = "dodge",stat="identity")+ coord_flip()+ xlab("Dataset [Algorithm]")
dev.off()
df<-read.csv('F.csv',header=T)
library(reshape2)
dfm <- melt(df[,c('Dataset...Algorithm.','Org','B.Bag','B.Ins')],id.vars = 1)
colnames(dfm) <- c("Dataset.Algorithm.", "variable","F")
library(ggplot2)
library(Cairo)
Cairo(file="F.png",type="png",units="in",width=10,height=7,pointsize=12,dpi=300)
ggplot(dfm,aes(x = Dataset.Algorithm. ,y = F)) +
geom_bar(aes(fill = variable),position = "dodge",stat="identity")+ coord_flip()+ xlab("Dataset [Algorithm]")
dev.off()
df<-read.csv('F.csv',header=T)
library(reshape2)
dfm <- melt(df[,c('Dataset...Algorithm.','Org','B.Bag','B.Ins')],id.vars = 1)
colnames(dfm) <- c("Dataset.Algorithm.", "variable","F")
library(ggplot2)
#library(Cairo)
#Cairo(file="F.png",type="png",units="in",width=10,height=7,pointsize=12,dpi=300)
ggplot(dfm,aes(x = Dataset.Algorithm. ,y = F)) +
geom_bar(aes(fill = variable),position = "dodge",stat="identity")+ coord_flip()+ xlab("Dataset [Algorithm]")
#dev.off()
qnorm(0.75,mean=1020,sd=50)
qnorm(0.95,mean=1100,sd=75)
round(qnorm(0.95,mean=1100,sd=75))
round(qnorm(0.95,mean=1100,sd=100/50))
round(qnorm(0.95,mean=100,sd=100/50))
(qnorm(0.95,mean=100,sd=100/50))
(qnorm(0.95,mean=100,sd=100/50))
(qnorm(0.95,mean=1100,sd=72/10))
round(qnorm(0.95,mean=1100,sd=72/10))
pnorm(1160,mean=1020,sd=50,lower.taiL=FASE)
pnorm(1160,mean=1020,sd=50,lower.taiL=FALSE)
pnorm(1160,mean=1020,sd=50)
pnorm(1160,mean=1020,sd=50,lower.tail=FALSE)
pnorm(1160,mean=1020,sd=50,lower.taiL=FALSE)
pnorm(1160,mean=1020,sd=50,lower.tail=FALSE)
pnorm(70,mean=80,sd=10,lower.tail=FALSE)
1-0.8
pnorm(70,mean=80,sd=10)
pnorm(70,mean=80,sd=10,lower.tail=true)
pnorm(70,mean=80,sd=10,lower.tail=TRUE)
round(pnorm(70,mean=80,sd=10,lower.tail=TRUE))*100
(pnorm(70,mean=80,sd=10,lower.tail=TRUE)*100
)
ppois(3,lambda=2.5*4)
ppois(20,lambda=16.5*2)
round(ppois(20,lambda=16.5*2)*100)
ppois(10,lambda=5*3)
round(ppois(10,lambda=5*3),2)
(168+179+198+184+187+182+155+145+167+145)/10
A=c(168,179,198,184,187,182,155,145,167,145)
A
mean(A)
sd(A)
mean(A)+2.821*(sd(A)/sqrt(10))
mean(A)-2.821*(sd(A)/sqrt(10))
B=(250, 240, 260, 245, 268, 279, 298, 284, 280 ,282)
B=c(250, 240, 260, 245, 268, 279, 298, 284, 280 ,282)
mean(B)-2.262*(sd(B)/sqrt(10))
mean(B)+2.262*(sd(B)/sqrt(10))
1100-2.306(30/sqrt(9))
1100-2.306*(30/sqrt(9))
1100+2.306*(30/sqrt(9))
6/(2.306)
data(sleep)
A
View(df)
data(sleep)
head(sleep)
View(sleep)
?qt
sp<-sqrt((9*0.6^2 + 9*0.68^2)/(10+10-2))
3-5+c(-1,1)*qt(.95,18)*sp*(1/10 + 1/19)^0.5
3-5+c(-1,1)*qt(.95,18)*sp*(1/10 + 1/10)^0.5
sp<-sqrt((9*0.6^2 + 9*0.68^2)/(10+10-2))
3-5+c(-1,1)*qt(.975,18)*sp*(1/10 + 1/10)^0.5
3-5+c(-1,1)*qt(.975,18)*sp*(1/10 + 1/10)^0.5
sp<-sqrt((9*0.6^2 + 9*0.68^2)/(10+10-2))
3-5+c(-1,1)*qt(.975,18)*sp*(1/10 + 1/10)^0.5
5-3+c(-1,1)*qt(.975,18)*sp*(1/10 + 1/10)^0.5
3-5+c(-1,1)*qt(.975,18)*sp*(1/10 + 1/10)^0.5
3-5+c(-1,1)*qt(.975,18)*sp*(1/10 + 1/10)^0.5
sp<-sqrt((99*1.41^2 + 99*0.7^2)/(100+100-2))
4-6+c(-1,1)*qt(.975,198)*sp*(1/100 + 1/100)^0.5
sp<-sqrt((9*0.6 + 9*0.68)/(10+10-2))
3-5+c(-1,1)*qt(.975,18)*sp*(1/10 + 1/10)^0.5
sp<-sqrt((8*1.2^2 + 8*1.3^2)/(9+9-2))
-3-1+c(-1,1)*qt(.95,16)*sp*(1/9 + 1/9)^0.5
sp<-sqrt((8*1.2^2 + 8*1.3^2)/(9+9-2))
-3-1+c(-1,1)*qt(.95,16)*sp*(1/9 + 1/9)^0.5
sp<-sqrt((8*1.5^2 + 8*1.8^2)/(9+9-2))
-3-1+c(-1,1)*qt(.95,16)*sp*(1/9 + 1/9)^0.5
hist(runif(1000))
mns = NULL
for (i in 1 : 1000) mns = c(mns, mean(runif(40)))
hist(mns)
hist(runif(1000))
runif(1000)
hist(runif(1000))
mns = NULL
for (i in 1 : 1000) mns = c(mns, mean(runif(40)))
hist(mns)
mns
rexp(40, 0.2)
hist(rexp(40, 0.2))
mne = NULL
for (i in 1 : 1000) mne = c(mne, mean(rexp(40, 0.2)))
hist(mne)
rexp(40, 0.2)
mean(rexp(40, 0.2))
load(ToothGrowth data)
load(ToothGrowth data)
load('ToothGrowth data')
load('ToothGrowth')
load(ToothGrowth)
hist(rexp(40, 0.2))
mne = NULL
for (i in 1 : 1000) mne = c(mne, mean(rexp(40, 0.2)))
hist(mne)
hist(rexp(40, 0.2))
mne = NULL
for (i in 1 : 1000) mne = c(mne, mean(rexp(400, 0.2)))
hist(mne)
1/0.2
vre = NULL
for (i in 1 : 1000) vre = c(vre, var(rexp(40, 0.2)))
hist(vre)
40/0.2
var(rexp(40, 0.2)
)
0.2^2
1/0.2^2
vre = NULL
for (i in 1 : 1000) vre = c(vre, var(rexp(400, 0.2)))
hist(vre)
abline(v=25,lwd=2)
abline(v=25,lwd=2,col=red)
abline(v=25,lwd=2,color=red)
abline(v=25,lwd=3)
vre = NULL
for (i in 1 : 1000) vre = c(vre, var(rexp(400, 0.2)))
hist(vre)
abline(v=25,lwd=3)
vre = NULL
for (i in 1 : 1000) vre = c(vre, var(rexp(40, 0.2)))
hist(vre)
abline(v=25,lwd=3)
vre = NULL
for (i in 1 : 1000) vre = c(vre, var(rexp(40, 0.2)))
hist(vre)
abline(v=25,lwd=3)
layout(matrix(c(1,1), 2, 2, byrow = TRUE))
vre = NULL
for (i in 1 : 1000) vre = c(vre, var(rexp(40, 0.2)))
hist(vre)
abline(v=25,lwd=3)
layout(matrix(c(1,1), 2, 2, byrow = TRUE))
vre = NULL
for (i in 1 : 1000) vre = c(vre, var(rexp(40, 0.2)))
hist(vre)
abline(v=25,lwd=3)
vre = NULL
for (i in 1 : 1000) vre = c(vre, var(rexp(40, 0.2)))
hist(vre)
mne = NULL
for (i in 1 : 1000) mne = c(mne, mean(rexp(40, 0.2)))
hist(mne)
layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE))
vre = NULL
for (i in 1 : 1000) vre = c(vre, var(rexp(40, 0.2)))
hist(vre)
vre = NULL
for (i in 1 : 1000) vre = c(vre, var(rexp(40, 0.2)))
hist(vre)
vre = NULL
for (i in 1 : 1000) vre = c(vre, var(rexp(40, 0.2)))
hist(vre)
layout(matrix(c(1,1), 2, 2, byrow = TRUE))
vre = NULL
for (i in 1 : 1000) vre = c(vre, var(rexp(40, 0.2)))
hist(vre)
vre = NULL
for (i in 1 : 1000) vre = c(vre, var(rexp(40, 0.2)))
hist(vre)
layout(matrix(c(1,1,2,2), 2, 2, byrow = TRUE))
vre = NULL
for (i in 1 : 1000) vre = c(vre, var(rexp(40, 0.2)))
hist(vre)
vre = NULL
for (i in 1 : 1000) vre = c(vre, var(rexp(40, 0.2)))
hist(vre)
layout(matrix(c(1,2,1,2), 2, 2, byrow = TRUE))
vre = NULL
for (i in 1 : 1000) vre = c(vre, var(rexp(40, 0.2)))
hist(vre)
vre = NULL
for (i in 1 : 1000) vre = c(vre, var(rexp(40, 0.2)))
hist(vre)
layout(matrix(c(1,1,2,2), 2, 2, byrow = TRUE))
vre = NULL
for (i in 1 : 1000) vre = c(vre, var(rexp(40, 0.2)))
hist(vre)
layout(matrix(c(1,1,2,2), 2, 2, byrow = TRUE))
vre = NULL
for (i in 1 : 1000) vre = c(vre, var(rexp(40, 0.2)))
hist(vre)
vre = NULL
for (i in 1 : 1000) vre = c(vre, var(rexp(40, 0.2)))
hist(vre)
abline(v=25,lwd=3)
title: "Untitled"
output: pdf_document
---
This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.
When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:
When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:
When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:
When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:
```{r, echo=FALSE}
plot(cars)
```
install.packages("pdflatex")
install.packages("texmex")
install.packages("pdfetch")
install.packages("TexLive")
clear
clear
load(ToothGrowth)
data(ToothGrowth)
str(ToothGrowth)
View(ToothGrowth)
plot(ToothGrowth$len,ToothGrowth$dose)
clear
cleat
plot(ToothGrowth$len,ToothGrowth$dose)
plot(ToothGrowth$len,ToothGrowth$dose)
plot(ToothGrowth$len,ToothGrowth$dose,col=supp)
plot(ToothGrowth$len,ToothGrowth$dose,col="supp")
plot(ToothGrowth)
boxplot(ToothGrowth)
boxplot(len~supp,d=ToothGrowth)
plot(ToothGrowth$len,ToothGrowth$sup)
plot(ToothGrowth$len,ToothGrowth$dose)
plot(ToothGrowth$dose,ToothGrowth$len)
plot(ToothGrowth$sup,ToothGrowth$len)
plot(ToothGrowth$sup,ToothGrowth$len)
summary(ToothGrowth)
tapply(ToothGrowth$len,ToothGrowth$sup,mean)
sp<-sqrt((8*1.2^2 + 8*1.3^2)/(9+9-2))
VC->subset(ToothGrowth,supp="VC")
VC->subset(ToothGrowth,supp=="VC")
VC->subset(ToothGrowth,supp=="OJ")
VC->subset(ToothGrowth,supp=="OJ")
VC->subset(ToothGrowth,supp=="OJ")
VC->subset(ToothGrowth,supp==VC)
VC->subset(ToothGrowth,supp=='VC')
VC->subset(ToothGrowth,supp=='OJ')
str(ToothGrowth)
VC->subset(ToothGrowth,supp==2)
VC->subset(ToothGrowth,supp==2)
#T test for the difference in mean in pig with VC and OJ.
VC<-subset(ToothGrowth,supp=='VC')
OJ<-subset(ToothGrowth,supp=='OJ')
tapply(ToothGrowth$len,ToothGrowth$sup,mean)
30+30
30+30-2
Sx<-var(VC$len)
Sx<-var(VC$len)
Sy<-<-var(OC$len)
Sx<-var(VC$len)
Sy<-var(OC$len)
Sx<-var(VC$len)
Sy<-var(OJ$len)
sp<-sqrt((29*Sx^2 + 29*Sy^2)/(30+30-2))
20.66333-16.96333+c(-1,1)*qt(.975,58)*sp*(1/30 + 1/30)^0.5
setwd("/Users/josemiguelarrieta/Documents/[Kaggle]Analytics_Edge_Competition")
# KAGGLE COMPETITION - DEALING WITH THE TEXT DATA
# This script file is intended to help you deal with the text data provided in the competition data files
# If you haven't already, start by reading the data into R
# Make sure you have downloaded these files from the Kaggle website, and have navigated to the directory where you saved the files on your computer
###########
#LOAD DATA#
###########
# We are adding in the argument stringsAsFactors=FALSE, since we have some text fields
eBayTrain = read.csv("Data/eBayiPadTrain.csv", stringsAsFactors=FALSE)
eBayTest = read.csv("Data/eBayiPadTest.csv", stringsAsFactors=FALSE)
# Now, let's load the "tm" package.
library(tm)
# Then create a corpus from the description variable. You can use other variables in the dataset for text analytics, but we will just show you how to use this particular variable.
# Note that we are creating a corpus out of the training and testing data.
CorpusDescription = Corpus(VectorSource(c(eBayTrain$description, eBayTest$description)))
CorpusDescription <- tm_map(CorpusDescription,
content_transformer(function(x) iconv(x, to='UTF-8-MAC', sub='byte')),
mc.cores=1)
# You can go through all of the standard pre-processing steps like we did in Unit 5:
CorpusDescription = tm_map(CorpusDescription, content_transformer(tolower), lazy=TRUE)
# Remember this extra line is needed after running the tolower step:
CorpusDescription = tm_map(CorpusDescription, PlainTextDocument, lazy=T)
CorpusDescription = tm_map(CorpusDescription, removePunctuation, lazy=T)
CorpusDescription = tm_map(CorpusDescription, removeWords, stopwords("english"), lazy=T)
#CorpusDescription = tm_map(CorpusDescription, stemDocument, lazy=T) #ERROR AQUI //Ya no (se arreglo)
# Now we are ready to convert our corpus to a DocumentTermMatrix, remove sparse terms, and turn it into a data frame.
# We selected one particular threshold to remove sparse terms, but remember that you can try different numbers!
dtm = DocumentTermMatrix(CorpusDescription)
sparse = removeSparseTerms(dtm, 0.99)
DescriptionWords = as.data.frame(as.matrix(sparse),row.names = NULL)
# Let's make sure our variable names are okay for R:
colnames(DescriptionWords) = make.names(colnames(DescriptionWords))
# Now we need to split the observations back into the training set and testing set.
# To do this, we can use the head and tail functions in R.
# The head function takes the first "n" rows of DescriptionWords (the first argument to the head function), where "n" is specified by the second argument to the head function.
# So here we are taking the first nrow(eBayTrain) observations from DescriptionWords, and putting them in a new data frame called "DescriptionWordsTrain"
DescriptionWordsTrain = head(DescriptionWords, nrow(eBayTrain))
# The tail function takes the last "n" rows of DescriptionWords (the first argument to the tail function), where "n" is specified by the second argument to the tail function.
# So here we are taking the last nrow(eBayTest) observations from DescriptionWords, and putting them in a new data frame called "DescriptionWordsTest"
DescriptionWordsTest = tail(DescriptionWords, nrow(eBayTest))
# Note that this split of DescriptionWords works to properly put the observations back into the training and testing sets, because of how we combined them together when we first made our corpus.
# Before building models, we want to add back the original variables from our datasets. We'll add back the dependent variable to the training set, and the WordCount variable to both datasets. You might want to add back more variables to use in your model - we'll leave this up to you!
#DescriptionWordsTrain$sold = eBayTrain$sold #Not necesary to add anymore.
DescriptionWordsTrain$WordCount = eBayTrain$WordCount
DescriptionWordsTest$WordCount = eBayTest$WordCount
# Remember that you can always look at the structure of these data frames to understand what we have created
#Approach: Join variables with text variables.
Data_Train<-cbind(eBayTrain,DescriptionWordsTrain)
Data_Test<-cbind(eBayTest,DescriptionWordsTest)
#Remove description, row.name columns and UniqueID Columns
Data_Train <- Data_Train[ -c(1,11) ]
rownames(Data_Train) <- NULL
Data_Test <- Data_Test[ -c(1,10) ]
rownames(Data_Test) <- NULL
#Apprach: Divide training Data into 2. Train and Test reals.
library(caTools)
set.seed(2000)
spl=sample.split(Data_Train$sold,SplitRatio=0.5)
train<-subset(Data_Train,spl==TRUE)
validate<-subset(Data_Train,spl==FALSE)
library(gplots)
library(rpart)
library(rpart.plot)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
Tree<-rpart(sold~., data = train,method="class",minbucket=25)
PredictCART= predict(Tree,newdata=validate[-c(9)],type="class")
predictROC=predict(Tree, newdata=validate[-c(9)], type="class") #Ignoring warning here
predictROC[is.na(predictROC)] <- 0   #0.65
pred=prediction(as.numeric(as.character(predictROC)),validate$sold)
table(train$condition)
table(validate$condition)
table(train$cellular)
table(validate$cellular)
table(train$carrier)
table(validate$carrier)
table(train$color)
#Cambiar aqui para ver si si da un AUC esprado
library(ROCR)
predictROC=predict(DescriptionWordsLog, newdata=validate[-c(9)], type="response") #Ignoring warning here
predictROC[is.na(predictROC)] <- 0   #0.65
#predictROC[is.na(predictLog)] <- 1     #0.75
predictROC<-as.numeric(predictROC >= 0.5)
pred=prediction(predictROC,validate$sold)
perf=performance(pred,"tpr","fpr")
plot(perf)
#AUC of the CART model
as.numeric(performance(pred, "auc")@y.values)
#############################################################################
# create a logistic regression model using all of the variables:
DescriptionWordsLog = glm(sold ~ ., data=train, family=binomial,control = list(maxit = 50))
summary(DescriptionWordsLog)
# And make predictions on our validation set:
PredTest = predict(DescriptionWordsLog, newdata=validate[-c(9)], type="response") #Ignoring Warning Message!
# Now we can prepare our submission file for Kaggle:
#DescriptionWordsLog = glm(sold ~ ., data=Data_Train, family=binomial)
#PredTest = predict(DescriptionWordsLog, newdata=Data_Test, type="response") #Ignoring Warning Message!
#MySubmission = data.frame(UniqueID = eBayTest$UniqueID, Probability1 = PredTest)
#write.csv(MySubmission, "SubmissionDescriptionLog2.csv", row.names=FALSE)
###################################################################################################
table(train$condition)
table(validate$condition)
table(train$cellular)
table(validate$cellular)
table(train$carrier)
table(validate$carrier)
table(train$color)
#Cambiar aqui para ver si si da un AUC esprado
library(ROCR)
predictROC=predict(DescriptionWordsLog, newdata=validate[-c(9)], type="response") #Ignoring warning here
predictROC[is.na(predictROC)] <- 0   #0.65
#predictROC[is.na(predictLog)] <- 1     #0.75
predictROC<-as.numeric(predictROC >= 0.5)
pred=prediction(predictROC,validate$sold)
perf=performance(pred,"tpr","fpr")
plot(perf)
#AUC of the CART model
as.numeric(performance(pred, "auc")@y.values)
str(predictROC)
predictROC=predict(Tree, newdata=validate[-c(9)], type="class") #Ignoring warning here
as.numeric(as.character(predictROC))
pred=prediction(as.numeric(as.character(predictROC)),validate$sold)
perf=performance(pred,"tpr","fpr")
plot(perf)
as.numeric(performance(pred, "auc")@y.values)
plot(perf)
as.numeric(performance(pred, "auc")@y.values)
TreeNaive<-rpart(sold~., data = Data_train,method="class",minbucket=25)
PredTest = predict(TreeNaive, newdata=Data_Test, type="response")
MySubmission = data.frame(UniqueID = eBayTest$UniqueID, Probability1 = PredTest)
write.csv(MySubmission, "SubmissionTree.csv", row.names=FALSE)
TreeNaive<-rpart(sold~., data = Data_train,method="class",minbucket=25)
TreeNaive<-rpart(sold~., data = Data_Train,method="class",minbucket=25)
PredTest = predict(TreeNaive, newdata=Data_Test, type="response")
PredTest = predict(TreeNaive, newdata=Data_Test, type="class")
MySubmission = data.frame(UniqueID = eBayTest$UniqueID, Probability1 = PredTest)
write.csv(MySubmission, "SubmissionTree.csv", row.names=FALSE)
